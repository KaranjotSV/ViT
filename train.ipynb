{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8aeaeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from layers import augmentation, Patches, PatchEncoder, mlp\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a811938",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 100\n",
    "in_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44227b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: (50000, 32, 32, 3) - labels: (50000, 1)\n",
      "testing: (10000, 32, 32, 3) - labels: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f'training: {x_train.shape} - labels: {y_train.shape}')\n",
    "print(f'testing: {x_test.shape} - labels: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80084740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 21:00:52.996571: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 153600000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97cee382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ViT():\n",
    "    inputs = layers.Input(in_shape)\n",
    "    augmented = augmentation(inputs)  # augment data\n",
    "    patches = Patches(patch_size)(augmented)  # create patches\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)  # encode patches\n",
    "\n",
    "    # create multiple layers of transformer block\n",
    "    for _ in range(transformer_layers):\n",
    "        # normalization 1\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # create a multi-head attention layer\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # skip connection 1\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # normalization 2\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP\n",
    "        x3 = mlp(x3, hidden=transformer_units, dropout=0.1)\n",
    "        # skip connection 2\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # create a [batch_size, projection_dim] tensor\n",
    "    rep = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    rep = layers.Flatten()(rep)\n",
    "    rep = layers.Dropout(0.5)(rep)\n",
    "\n",
    "    # MLP\n",
    "    features = mlp(rep, hidden=mlp_head, dropout=0.5)\n",
    "    # classify\n",
    "    logits = layers.Dense(classes)(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969a50a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Miscellaneous",
   "language": "python",
   "name": "miscellaneous"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
